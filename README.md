# Summer-Internship-II: IBM AI Engineering Professional Certificate

## ðŸ“Œ Overview
This repository showcases my work and learning progress during my Summer Internship focused on Machine Learning, Deep Learning, Generative AI, LLMs, and AI Agents. The training program consisted of 13 intensive courses (~166 hours), combining theory, hands-on projects and real-world AI applications.

## ðŸŽ¯ Learning Objectives

- Build strong foundations in Machine Learning and Deep Learning using Python, Keras, TensorFlow, and PyTorch.
- Understand and implement Generative AI concepts, including LLM architecture, fine-tuning, and prompt engineering.
- Gain practical experience with LangChain and Retrieval-Augmented Generation (RAG) for AI agents.
- Develop real-world AI projects from scratch to deployment-ready state.

## ðŸ“š Courses Completed

I. Machine Learning & Deep Learning Foundations

1. Machine Learning with Python (20 hrs) â€“ Built and evaluated end-to-end ML solutions using Python & scikit-learn, including supervised/unsupervised learning, regression, classification, clustering, dimensionality reduction, and optimization.

2. Introduction to Deep Learning & Neural Networks with Keras (10 hrs) â€“ Learned neural network architecture, CNNs, RNNs, transformers, and model evaluation using Keras.
   
3. Deep Learning with Keras and TensorFlow (23 hrs) â€“ Developed custom models integrating Keras with TensorFlow, implemented CNNs, transformers, reinforcement learning, and unsupervised learning methods.
   
4. Introduction to Neural Networks and PyTorch (17 hrs) â€“ Implemented regression models from scratch in PyTorch, applied logistic regression, gradient descent optimization, and trained neural networks for classification.
   
5. Deep Learning with PyTorch (20 hrs) â€“ Trained deep neural networks with dropout, weight initialization, batch normalization, and CNN architectures in PyTorch.
    
6. AI Capstone Project with Deep Learning (14 hrs) â€“ Designed a complete deep learning pipeline with Keras/PyTorch, applying CNNs and vision transformers for geospatial land classification.

II. Generative AI & NLP

7. Generative AI and LLMs: Architecture and Data Preparation (5 hrs) â€“ Learned architectures of RNNs, transformers, VAEs, GANs, diffusion models, and text preprocessing with NLP libraries.

8. Gen AI Foundational Models for NLP & Language Understanding (10 hrs) â€“ Built NLP features (one-hot, embeddings, Word2Vec), statistical N-Grams, and sequence-to-sequence RNN models for translation.

9. Generative AI Language Modeling with Transformers (9 hrs) â€“ Implemented transformer models (GPT, BERT) with attention mechanisms using PyTorch & Hugging Face for NLP tasks.

10. Generative AI Engineering and Fine-Tuning Transformers (8 hrs) â€“ Applied parameter-efficient fine-tuning (LoRA, QLoRA) and optimized pretrained transformer models for domain-specific tasks.

11. Generative AI Advance Fine-Tuning for LLMs (9 hrs) â€“ Used RLHF, DPO, and PPO techniques to fine-tune LLMs with Hugging Face for scoring and dataset preparation.

III. AI Agents & Applied Generative AI

12. Fundamentals of AI Agents Using RAG and LangChain (7 hrs) â€“ Built AI agents with RAG, LangChain, Hugging Face, and PyTorch, applying prompt engineering and in-context learning.

13. Project: Generative AI Applications with RAG and LangChain (9 hrs) â€“ Developed a question-answering bot with LangChain & LLMs, using vector databases for embeddings, retrievers, and a Gradio interface for interaction.

## ðŸ›  Skills Acquired

- Languages & Libraries: Python, scikit-learn, Keras, TensorFlow, PyTorch, Hugging Face, LangChain, Gradio
- ML/DL Models: Regression, Classification, Clustering, CNN, RNN, Transformers, Reinforcement Learning
- Generative AI: LLMs, Fine-tuning (LoRA, QLoRA, RLHF), Prompt Engineering, RAG pipelines
- Data Handling: Data preprocessing, embeddings, vector databases, unstructured data processing
- Deployment Tools: Gradio, LangChain, API integration

## Course Link: https://listwr.com/NGFrDF
